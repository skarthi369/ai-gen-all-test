Step 1: Tokenization (breaking text into pieces)

Your input sentence was:
ğŸ‘‰ "Hello world! This is a BERT tokenization demo."

BERT doesnâ€™t read text as words. It breaks it into tiny pieces called tokens:

['hello', 'world', '!', 'this', 'is', 'a', 'bert', 'token', '##ization', 'demo', '.']


Notice tokenization got split into token + ##ization â†’ BERT does this for unknown words.

Then each token is turned into a number (ID) so the computer can understand it:

[7592, 2088, 999, 2023, ...]


Special tokens are added:

[CLS] â†’ start of sentence

[SEP] â†’ end of sentence

[PAD] â†’ filler if sentence is shorter than max length

ğŸ§® Step 2: Embeddings (turning tokens into vectors)

Each token is mapped to a 768-dimensional vector (a list of 768 numbers).
These numbers capture meaning, like â€œhelloâ€ and â€œhiâ€ having similar vectors.

There are 3 embeddings:

Token embeddings â†’ meaning of each word

Position embeddings â†’ where the word is in the sentence

Segment embeddings â†’ which sentence it belongs to (for Q&A tasks)

All combined into:
ğŸ‘‰ Shape = [1, 512, 768]

1 = batch size (1 sentence)

512 = max tokens BERT allows

768 = hidden size (vector length for each token)

ğŸ”„ Step 3: Transformer Layers (deep thinking steps)

BERT has 12 layers.
Each layer processes the embeddings and refines them.
The report shows how the numbers change layer by layer:

Layer 0: mean=-0.065, std=0.492
Layer 1: mean=-0.019, std=0.494
...


That just means the sentence is being transformed into a better representation of meaning.

ğŸ‘ï¸ Step 4: Attention (what BERT focuses on)

BERT doesnâ€™t just read left to right â€” it looks at all words at once.
Attention tells us: â€œwhich words matter most when understanding this token?â€

For example:

[CLS] (the summary token) looks at:

. (attention 0.336)

[SEP] (0.317)

[CLS] itself (0.157)

bert (0.039)

demo (0.035)

This means when summarizing the whole sentence, BERT is paying most attention to punctuation and keywords.

ğŸ¯ Final output

Final hidden states shape: [1, 512, 768] â†’ the final meaning vectors for each token.

Attention weights shape: [1, 12, 512, 512] â†’ 12 attention heads, each showing how every word relates to every other word.

âœ… In plain words:
You gave BERT a sentence.
BERT split it into tokens, turned them into numbers, processed them through 12 layers of "deep thinking," and paid attention to relationships between words.
The result is a rich numerical representation of your sentence, which can then be used for tasks like classification, Q&A, or text similarity.